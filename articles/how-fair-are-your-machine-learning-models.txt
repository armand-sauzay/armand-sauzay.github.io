1:"$Sreact.fragment"
2:I[48173,["173","static/chunks/173-24113e259ce525da.js","292","static/chunks/app/articles/page-9a53c5301fbc680c.js"],""]
3:I[15244,[],""]
4:I[43866,[],""]
6:I[86213,[],"OutletBoundary"]
8:I[86213,[],"MetadataBoundary"]
a:I[86213,[],"ViewportBoundary"]
c:I[34835,[],""]
:HL["/_next/static/media/a34f9d1faa5f3315-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/27c4eb1be0a8ef66.css","style"]
0:{"P":null,"b":"PQfJs3TWpIcT2jSkPpTci","p":"","c":["","articles","how-fair-are-your-machine-learning-models"],"i":false,"f":[[["",{"children":["articles",{"children":[["slug","how-fair-are-your-machine-learning-models","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/27c4eb1be0a8ef66.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__className_d65c78 min-h-screen flex flex-col","children":[["$","div",null,{"children":[["$","nav",null,{"className":"p-6","children":["$","div",null,{"className":"max-w-4xl mx-auto flex justify-between items-center","children":[["$","$L2",null,{"href":"/","className":"text-[#005530] font-semibold text-xl","children":"armandsauzay"}],["$","div",null,{"className":"flex gap-6","children":[["$","$L2",null,{"href":"/articles","className":"text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-book-open h-5 w-5","children":[["$","path","1akyts",{"d":"M12 7v14"}],["$","path","ruj8y",{"d":"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z"}],"$undefined"]}],["$","span",null,{"children":"Articles"}]]}],["$","$L2",null,{"href":"/projects","className":"text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-code-xml h-5 w-5","children":[["$","path","1inbqp",{"d":"m18 16 4-4-4-4"}],["$","path","15zrgr",{"d":"m6 8-4 4 4 4"}],["$","path","e7oirm",{"d":"m14.5 4-5 16"}],"$undefined"]}],["$","span",null,{"children":"Projects"}]]}],["$","$L2",null,{"href":"/creative-lab","className":"text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-brain h-5 w-5","children":[["$","path","l5xja",{"d":"M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z"}],["$","path","ep3f8r",{"d":"M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z"}],["$","path","1p4c4q",{"d":"M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4"}],["$","path","tmeiqw",{"d":"M17.599 6.5a3 3 0 0 0 .399-1.375"}],["$","path","105sqy",{"d":"M6.003 5.125A3 3 0 0 0 6.401 6.5"}],["$","path","ql3yin",{"d":"M3.477 10.896a4 4 0 0 1 .585-.396"}],["$","path","1qfode",{"d":"M19.938 10.5a4 4 0 0 1 .585.396"}],["$","path","2e4loj",{"d":"M6 18a4 4 0 0 1-1.967-.516"}],["$","path","159ez6",{"d":"M19.967 17.484A4 4 0 0 1 18 18"}],"$undefined"]}],["$","span",null,{"children":"Creative Lab"}]]}]]}]]}]}],["$","div",null,{"className":"max-w-4xl mx-auto border-b border-gray-200"}]]}],["$","main",null,{"className":"flex-1 flex flex-col items-center justify-center","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"w-full flex justify-center items-center gap-4 p-6 text-sm text-gray-500 mt-auto","children":[["$","div",null,{"className":"flex gap-8 items-center ","children":[["$","$L2",null,{"href":"https://github.com/armand-sauzay","className":"text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 ","target":"_blank","rel":"noopener noreferrer","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","style":{"width":16,"height":16},"children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"GitHub"}]]}],["$","$L2",null,{"href":"https://linkedin.com/in/armandsauzay","className":"text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 ","target":"_blank","rel":"noopener noreferrer","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-linkedin h-4 w-4","style":{"width":16,"height":16},"children":[["$","path","c2jq9f",{"d":"M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"}],["$","rect","mk3on5",{"width":"4","height":"12","x":"2","y":"9"}],["$","circle","bt5ra8",{"cx":"4","cy":"4","r":"2"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"LinkedIn"}]]}],["$","$L2",null,{"href":"mailto:sauzayarmand@gmail.com","className":"text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 ","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-mail h-4 w-4","style":{"width":16,"height":16},"children":[["$","rect","18n3k1",{"width":"20","height":"16","x":"2","y":"4","rx":"2"}],["$","path","1ocrg3",{"d":"m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"Email"}]]}]]}],["$","span",null,{"className":"text-gray-400","children":"Â© armand sauzay"}]]}]]}]}]]}],{"children":["articles",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","articles","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","how-fair-are-your-machine-learning-models","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","articles","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",null,["$","$L6",null,{"children":"$L7"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","GPNQ5qHwD7UZMwzQ1MM2C",{"children":[["$","$L8",null,{"children":"$L9"}],["$","$La",null,{"children":"$Lb"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$c","$undefined"],"s":false,"S":true}
b:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Armand Sauzay"}],["$","meta","2",{"name":"description","content":"Personal website of Armand Sauzay"}],["$","link","3",{"rel":"icon","href":"/favicon.ico"}]]
7:null
d:I[95958,["313","static/chunks/313-6cc61c2fd8ca5084.js","904","static/chunks/app/articles/%5Bslug%5D/page-444914622aa9eeee.js"],"default"]
e:T2804,# How Fair Are Your Machine LearningÂ Models?

A quick introduction to the topic of fairness with hands on coding. Evaluate your machine learning model fairness in just a few lines ofÂ code.

This article lives on: 
- [Medium](https://armand-sauzay.medium.com/a-primer-on-machine-learning-fairness-using-fairlearn-8c00f92015dc)
- [Dev.to](https://dev.to/armandsauzay/machine-learning-interpretability-and-feature-selection-made-easy-with-shap-450c)

![Photo by Wesley Tingey onÂ Unsplash](https://miro.medium.com/max/1400/1*MZWIZHRWNqGkyoC_MKT-OA.webp)
<p align="center"> Photo by Wesley Tingey onÂ Unsplash </p>


Are Machine Learning models "fair"? When increasingly more decisions are backed by ML algorithms, it becomes important to understand the biases they can create.

But what does "fairness" mean? This is where it gets a little political (and mathematical)â¦ To illustrate our thoughts, we'll take the example of a machine learning model which predicts whether a salary should be higher than 50K/year based on a number of features including __age__ and __gender__.

And maybe you've already guessed, by looking at these two features, that fairness can have different definitions. Fair for __gender__ might mean that we want to have the a prediction which is independent of gender (i.e. paying the same people who only differ by their gender). Fair for __age__ might mean something else. We'd probably want to allow a certain correlation between the prediction and the age, as it seems fair to pay better older individuals (which usually are more experienced).

One key thing to understand is that what is judged "fair" is sometimes not even respected in the data itself.

> How would the model learn that men and women should be paid the same at equal levels it it does not observe this in the data itselfÂ ?

![data biases vs modelÂ biases](https://miro.medium.com/max/1400/1*W_rBWQ-Z1Mw3sE43lDfNLw.webp)
<p align="center"> Figure1: data biases vs modelÂ biases </p>

Now that we have a bit of context on the problem, let's get into the math (Section 1) and the code (Sections 2 and 3)to be able to evaluate and address unfairness issues:

1.  A few fairness concepts
2. Evaluating Data Biases
3. Evaluating and Correcting Model Biases with Fairlearn   
    
    a. Evaluating biasÂ 

    b. Correcting bias

---

All the code for this tutorial can be found on Kaggle [here](https://www.kaggle.com/code/armandsauzay/a-primer-on-fairness-with-fairlearn). Feel free to run the notebook yourself or create aÂ copy!

---

## 1. A few fairness concepts
### 1.1. Mathematical definition of fairness 
In order to simplify things, we'll restrict the scope to binary classification (predict whether someone should be paid more than 50K/year).
Usually, we'll call:
- X: the feature matrix
- Y: the target
- A: Sensitive feature, usually one of the columns of X

For __binary classification__, two main definition of fairness exist:
- __Demographic parity__ (also known as statistical parity): A classifier h satisfies demographic parity under a distribution over (X,A,Y) if its prediction h(X) is statistically independent of the sensitive feature A. This is equivalent to: `E[h(X)|A=a]=E[h(X)]`
- __Equalized odds__: A classifier h satisfies equalized odds under a distribution over (X,A,Y) if its prediction h(X) is conditionally independent of the sensitive feature A given the label Y. This is equivalent to: `E[h(X)|A=a,Y=y]=E[h(X)|Y=y]`

> NOTE: a third one exists but is more rarely used: __equal opportunity__ is a relaxed version of equalized odds that only considers conditional expectations with respect to positive labels.

### 1.2. Fairness in words

In "simpler words":
- __Demographic parity__: the prediction should be independent from the sensitive features (for instance independent from gender). It states that all categories from the protected feature should receive the positive outcome at the same rate (it plays on selection rate)
- __Equalized odds__: the prediction can be correlated to the sensitive feature, to the extent it is explained by the data we see

### 1.3. Why does it matter?

> OK, that's interesting, but why does it matter? And how can I use those mathematical concepts?

â Let's take two examples of features and then explain what type of fairness we want to have for this feature. Going back to the previous example of salary prediction, let's say you are the CEO of a very big company and want to build an algorithm which would give you the salary you should give to your employees based on performance indicators. Ideally you would look for something like:
- Demographic Parity for gender: the salary prediction should be independent from the gender
- Equalized Odds for Age: the salary prediction should not be independent from Age (you want to still pay more employees with more experience) but you still want to control that the salary so that you do not end up being too skewed â you don't want to end up in the situation where the algorithm exacerbates even more the inequalities (pays the youth even less and the elders even more)

Without further due, let's get into the implementation details on how we can evaluate fairness and "retrain" our Machine Learning models against its biases. For this we're going to use the [UCI Adult Dataset](https://archive.ics.uci.edu/ml/datasets/adult).

## 2. Evaluating DataÂ Biases
_NOTE: once again, you can find all the associated code [here](https://www.kaggle.com/code/armandsauzay/a-primer-on-fairness-with-fairlearn)._ 

Biases can exist in the data itself. Let's just load the data and plot the percentage of Male/Female having a salary above 50K.


<p><img src="https://miro.medium.com/max/1106/1*dMFb4gM4LjtQ5TC52fBTVA.webp" width="400" height="400" /> <img src="https://miro.medium.com/max/896/1*d5GJ2Z7Yz3uom5ANr96yGQ.webp" width="400" height="400" /> </p>
<p align="center"> Figure 2: gender and age impact onÂ salary </p>

We see that the percentage of males having a salary above 50K is almost 3x the percentage of females. (!!)

If the algorithm learns on this data it will definitely be biased. To counter this bias we can either:Â 
1. cherry pick data so that the percentage of maleÂ 
2. use fairlearn to correct the bias after the model is trained on this unfair data

In section 3, we'll focus on the second approach.


## 3. Evaluating and Correcting Model Biases with Fairlearn
### 3.1. Evaluating bias
One of the most interesting features here is probably __selection rate__. It is the rate of predicting positive outcomes (in this case, whether salary is above 50K)

<p align="center"> <img src="https://miro.medium.com/max/708/1*oflHCW6PPNRa2Eb0yAIjvA.webp" /> </p>
<p align="center"> Figure 3: Selection Rate Definition </p>

Let's use MetricFrame from fairlearn to calculate the selection rates split by Sex.

```python
from fairlearn.metrics import MetricFrame
from sklearn.metrics import accuracy_score,precision_score,recall_score
from sklearn.ensemble import GradientBoostingClassifier
from fairlearn.metrics import selection_rate
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
classifier = GradientBoostingClassifier()
classifier.fit(X, y)
y_pred = classifier.predict(X)
metrics = {
    'accuracy': accuracy_score,
    'precision': precision_score,
    'recall': recall_score,
    'selection_rate': selection_rate
}
metric_frame = MetricFrame(metrics=metrics,
                           y_true=y,
                           y_pred=y_pred,
                           sensitive_features=sex)
metric_frame.by_group['selection_rate'].plot.bar(color= 'r', title='Selection Rate split by Sex')
```

![Selection Rate Split by Sex](https://miro.medium.com/max/1400/1*ILbeO5Vn4AsXPfgEZeLCiA.webp)
<p align='center'> Figure 4: Selection Rate Split by Sex </p>

We see that the percentage of males having a salary above 50K is almost 3x the percentage of females. (!!)
Once the model is trained we see that this ratio is now 5x (!!). The model is exacerbating the bias we see in the data.

### 3.2. Correcting bias
Let's now correct the bias we observe by applying Demographic Parity on our classifier (we use ExponentiatedGradient from fairlearn for this). More context on how it works behind the scene can be found on the official fairlearn documentation here.

```python
np.random.seed(0)  # set seed for consistent results with ExponentiatedGradient

constraint = DemographicParity()
classifier = GradientBoostingClassifier()
mitigator = ExponentiatedGradient(classifier, constraint)
mitigator.fit(X, y, sensitive_features=sex)
y_pred_mitigated = mitigator.predict(X)

sr_mitigated = MetricFrame(metrics=selection_rate, y_true=y, y_pred=y_pred_mitigated, sensitive_features=sex)
print(sr_mitigated.overall)
print(sr_mitigated.by_group)
metric_frame_mitigated = MetricFrame(metrics=metrics,
                           y_true=y,
                           y_pred=y_pred_mitigated,
                           sensitive_features=sex)
metric_frame_mitigated.by_group.plot.bar(
    subplots=True,
    layout=[3, 3],
    legend=False,
    figsize=[12, 8],
    title="Show all metrics",
)
```
![Selection rate for original model vs mitigated one](https://miro.medium.com/max/1400/1*9x24g1w-4HVrLr4nNQEPvw.webp)
<p align='center'> Figure 5: Selection rate for original model vs mitigated one </p>

By mitigating the model we introduced demographic parity (and thus equal selection rates) for our new model. Our model is now fair!!!

---

Woohoo! You now know the basics of fairness works and how you can start using it right away in your machine learning projects!

I hope you liked this article! Let me know if you have any questions or suggestions. Also feel free to contact me on LinkedIn, GitHub or Twitter, or checkout some other articles I wrote on DS/ML best practices. Happy learning!

Sources:
- https://fairlearn.org/

## About me
Hey! ð I'm Armand Sauzay ([armandsauzay](https://twitter.com/armandsauzay)). You can find, follow or contact me on: 

- [Github](https://github.com/armand-sauzay) 
- [Twitter](https://twitter.com/armandsauzay)
- [LinkedIn](https://www.linkedin.com/in/armand-sauzay-80a70b160/)
- [Medium](https://medium.com/@armand-sauzay)
- [Dev.to](https://dev.to/armandsauzay)5:["$","div",null,{"className":"min-h-screen flex flex-col","children":["$","main",null,{"className":"flex-1 max-w-4xl mx-auto px-4 py-16","children":["$","$Ld",null,{"markdown":"$e","markdownUrl":"https://raw.githubusercontent.com/armand-sauzay/blog-posts/main/how-fair-are-your-machine-learning-models/README.md"}]}]}]
