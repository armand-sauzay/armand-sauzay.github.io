<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="https://miro.medium.com/max/1400/1*MZWIZHRWNqGkyoC_MKT-OA.webp"/><link rel="preload" as="image" href="https://miro.medium.com/max/1400/1*W_rBWQ-Z1Mw3sE43lDfNLw.webp"/><link rel="preload" as="image" href="https://miro.medium.com/max/1106/1*dMFb4gM4LjtQ5TC52fBTVA.webp"/><link rel="preload" as="image" href="https://miro.medium.com/max/896/1*d5GJ2Z7Yz3uom5ANr96yGQ.webp"/><link rel="preload" as="image" href="https://miro.medium.com/max/708/1*oflHCW6PPNRa2Eb0yAIjvA.webp"/><link rel="preload" as="image" href="https://miro.medium.com/max/1400/1*ILbeO5Vn4AsXPfgEZeLCiA.webp"/><link rel="preload" as="image" href="https://miro.medium.com/max/1400/1*9x24g1w-4HVrLr4nNQEPvw.webp"/><link rel="stylesheet" href="/_next/static/css/56593b524a1ee8d7.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-87b4446f5aacfc58.js"/><script src="/_next/static/chunks/4bd1b696-d0010192273b2f08.js" async=""></script><script src="/_next/static/chunks/517-34c25e9c69b3332f.js" async=""></script><script src="/_next/static/chunks/main-app-a6b2e7fa0ef815fd.js" async=""></script><script src="/_next/static/chunks/173-24113e259ce525da.js" async=""></script><script src="/_next/static/chunks/app/articles/page-9a53c5301fbc680c.js" async=""></script><script src="/_next/static/chunks/313-6cc61c2fd8ca5084.js" async=""></script><script src="/_next/static/chunks/app/articles/%5Bslug%5D/page-444914622aa9eeee.js" async=""></script><meta name="next-size-adjust" content=""/><title>Armand Sauzay</title><meta name="description" content="Personal website of Armand Sauzay"/><link rel="icon" href="/favicon.ico"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d65c78 min-h-screen flex flex-col"><div><nav class="p-6"><div class="max-w-4xl mx-auto flex justify-between items-center"><a class="text-[#005530] font-semibold text-xl" href="/">armandsauzay</a><div class="flex gap-6"><a class="text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer" href="/articles"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-book-open h-5 w-5"><path d="M12 7v14"></path><path d="M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z"></path></svg><span>Articles</span></a><a class="text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer" href="/projects"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-code-xml h-5 w-5"><path d="m18 16 4-4-4-4"></path><path d="m6 8-4 4 4 4"></path><path d="m14.5 4-5 16"></path></svg><span>Projects</span></a><a class="text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer" href="/creative-lab"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-brain h-5 w-5"><path d="M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z"></path><path d="M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z"></path><path d="M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4"></path><path d="M17.599 6.5a3 3 0 0 0 .399-1.375"></path><path d="M6.003 5.125A3 3 0 0 0 6.401 6.5"></path><path d="M3.477 10.896a4 4 0 0 1 .585-.396"></path><path d="M19.938 10.5a4 4 0 0 1 .585.396"></path><path d="M6 18a4 4 0 0 1-1.967-.516"></path><path d="M19.967 17.484A4 4 0 0 1 18 18"></path></svg><span>Creative Lab</span></a></div></div></nav><div class="max-w-4xl mx-auto border-b border-gray-200"></div></div><main class="flex-1 flex flex-col items-center justify-center"><div class="min-h-screen flex flex-col"><main class="flex-1 max-w-4xl mx-auto px-4 py-16"><article class="prose prose-neutral max-w-2xl mx-auto p-6 prose-pre:border-0"><h1 id="how-fair-are-your-machine-learningmodels">How Fair Are Your Machine LearningÂ Models?</h1>
<p>A quick introduction to the topic of fairness with hands on coding. Evaluate your machine learning model fairness in just a few lines ofÂ code.</p>
<p>This article lives on:</p>
<ul>
<li><a href="https://armand-sauzay.medium.com/a-primer-on-machine-learning-fairness-using-fairlearn-8c00f92015dc">Medium</a></li>
<li><a href="https://dev.to/armandsauzay/machine-learning-interpretability-and-feature-selection-made-easy-with-shap-450c">Dev.to</a></li>
</ul>
<p><img src="https://miro.medium.com/max/1400/1*MZWIZHRWNqGkyoC_MKT-OA.webp" alt="Photo by Wesley Tingey onÂ Unsplash" node="[object Object]"/></p>
<p align="center"> Photo by Wesley Tingey onÂ Unsplash </p>
<p>Are Machine Learning models &quot;fair&quot;? When increasingly more decisions are backed by ML algorithms, it becomes important to understand the biases they can create.</p>
<p>But what does &quot;fairness&quot; mean? This is where it gets a little political (and mathematical)â€¦ To illustrate our thoughts, we&#x27;ll take the example of a machine learning model which predicts whether a salary should be higher than 50K/year based on a number of features including <strong>age</strong> and <strong>gender</strong>.</p>
<p>And maybe you&#x27;ve already guessed, by looking at these two features, that fairness can have different definitions. Fair for <strong>gender</strong> might mean that we want to have the a prediction which is independent of gender (i.e. paying the same people who only differ by their gender). Fair for <strong>age</strong> might mean something else. We&#x27;d probably want to allow a certain correlation between the prediction and the age, as it seems fair to pay better older individuals (which usually are more experienced).</p>
<p>One key thing to understand is that what is judged &quot;fair&quot; is sometimes not even respected in the data itself.</p>
<blockquote>
<p>How would the model learn that men and women should be paid the same at equal levels it it does not observe this in the data itselfÂ ?</p>
</blockquote>
<p><img src="https://miro.medium.com/max/1400/1*W_rBWQ-Z1Mw3sE43lDfNLw.webp" alt="data biases vs modelÂ biases" node="[object Object]"/></p>
<p align="center"> Figure1: data biases vs modelÂ biases </p>
<p>Now that we have a bit of context on the problem, let&#x27;s get into the math (Section 1) and the code (Sections 2 and 3)to be able to evaluate and address unfairness issues:</p>
<ol>
<li>
<p>A few fairness concepts</p>
</li>
<li>
<p>Evaluating Data Biases</p>
</li>
<li>
<p>Evaluating and Correcting Model Biases with Fairlearn</p>
<p>a. Evaluating biasÂ </p>
<p>b. Correcting bias</p>
</li>
</ol>
<hr/>
<p>All the code for this tutorial can be found on Kaggle <a href="https://www.kaggle.com/code/armandsauzay/a-primer-on-fairness-with-fairlearn">here</a>. Feel free to run the notebook yourself or create aÂ copy!</p>
<hr/>
<h2 id="1-a-few-fairness-concepts">1. A few fairness concepts</h2>
<h3 id="11-mathematical-definition-of-fairness">1.1. Mathematical definition of fairness</h3>
<p>In order to simplify things, we&#x27;ll restrict the scope to binary classification (predict whether someone should be paid more than 50K/year).
Usually, we&#x27;ll call:</p>
<ul>
<li>X: the feature matrix</li>
<li>Y: the target</li>
<li>A: Sensitive feature, usually one of the columns of X</li>
</ul>
<p>For <strong>binary classification</strong>, two main definition of fairness exist:</p>
<ul>
<li><strong>Demographic parity</strong> (also known as statistical parity): A classifier h satisfies demographic parity under a distribution over (X,A,Y) if its prediction h(X) is statistically independent of the sensitive feature A. This is equivalent to: <code node="[object Object]">E[h(X)|A=a]=E[h(X)]</code></li>
<li><strong>Equalized odds</strong>: A classifier h satisfies equalized odds under a distribution over (X,A,Y) if its prediction h(X) is conditionally independent of the sensitive feature A given the label Y. This is equivalent to: <code node="[object Object]">E[h(X)|A=a,Y=y]=E[h(X)|Y=y]</code></li>
</ul>
<blockquote>
<p>NOTE: a third one exists but is more rarely used: <strong>equal opportunity</strong> is a relaxed version of equalized odds that only considers conditional expectations with respect to positive labels.</p>
</blockquote>
<h3 id="12-fairness-in-words">1.2. Fairness in words</h3>
<p>In &quot;simpler words&quot;:</p>
<ul>
<li><strong>Demographic parity</strong>: the prediction should be independent from the sensitive features (for instance independent from gender). It states that all categories from the protected feature should receive the positive outcome at the same rate (it plays on selection rate)</li>
<li><strong>Equalized odds</strong>: the prediction can be correlated to the sensitive feature, to the extent it is explained by the data we see</li>
</ul>
<h3 id="13-why-does-it-matter">1.3. Why does it matter?</h3>
<blockquote>
<p>OK, that&#x27;s interesting, but why does it matter? And how can I use those mathematical concepts?</p>
</blockquote>
<p>â†’ Let&#x27;s take two examples of features and then explain what type of fairness we want to have for this feature. Going back to the previous example of salary prediction, let&#x27;s say you are the CEO of a very big company and want to build an algorithm which would give you the salary you should give to your employees based on performance indicators. Ideally you would look for something like:</p>
<ul>
<li>Demographic Parity for gender: the salary prediction should be independent from the gender</li>
<li>Equalized Odds for Age: the salary prediction should not be independent from Age (you want to still pay more employees with more experience) but you still want to control that the salary so that you do not end up being too skewed â†’ you don&#x27;t want to end up in the situation where the algorithm exacerbates even more the inequalities (pays the youth even less and the elders even more)</li>
</ul>
<p>Without further due, let&#x27;s get into the implementation details on how we can evaluate fairness and &quot;retrain&quot; our Machine Learning models against its biases. For this we&#x27;re going to use the <a href="https://archive.ics.uci.edu/ml/datasets/adult">UCI Adult Dataset</a>.</p>
<h2 id="2-evaluating-databiases">2. Evaluating DataÂ Biases</h2>
<p><em>NOTE: once again, you can find all the associated code <a href="https://www.kaggle.com/code/armandsauzay/a-primer-on-fairness-with-fairlearn">here</a>.</em></p>
<p>Biases can exist in the data itself. Let&#x27;s just load the data and plot the percentage of Male/Female having a salary above 50K.</p>
<p><img src="https://miro.medium.com/max/1106/1*dMFb4gM4LjtQ5TC52fBTVA.webp" width="400" height="400" node="[object Object]"/> <img src="https://miro.medium.com/max/896/1*d5GJ2Z7Yz3uom5ANr96yGQ.webp" width="400" height="400" node="[object Object]"/> </p>
<p align="center"> Figure 2: gender and age impact onÂ salary </p>
<p>We see that the percentage of males having a salary above 50K is almost 3x the percentage of females. (!!)</p>
<p>If the algorithm learns on this data it will definitely be biased. To counter this bias we can either:Â </p>
<ol>
<li>cherry pick data so that the percentage of maleÂ </li>
<li>use fairlearn to correct the bias after the model is trained on this unfair data</li>
</ol>
<p>In section 3, we&#x27;ll focus on the second approach.</p>
<h2 id="3-evaluating-and-correcting-model-biases-with-fairlearn">3. Evaluating and Correcting Model Biases with Fairlearn</h2>
<h3 id="31-evaluating-bias">3.1. Evaluating bias</h3>
<p>One of the most interesting features here is probably <strong>selection rate</strong>. It is the rate of predicting positive outcomes (in this case, whether salary is above 50K)</p>
<p align="center"> <img src="https://miro.medium.com/max/708/1*oflHCW6PPNRa2Eb0yAIjvA.webp" node="[object Object]"/> </p>
<p align="center"> Figure 3: Selection Rate Definition </p>
<p>Let&#x27;s use MetricFrame from fairlearn to calculate the selection rates split by Sex.</p>
<pre><div style="position:relative"><button aria-label="Copy code" style="position:absolute;top:8px;right:8px;z-index:10;background:none;border:none;padding:0;cursor:pointer;opacity:0.7;transition:opacity 0.2s"><svg width="20" height="20" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor"></rect><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path></svg></button><pre style="background:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;border-radius:0.3em"><code class="language-python" style="white-space:pre;background:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:hsl(301, 63%, 40%)">from</span><span> fairlearn</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>metrics </span><span class="token" style="color:hsl(301, 63%, 40%)">import</span><span> MetricFrame
</span><span></span><span class="token" style="color:hsl(301, 63%, 40%)">from</span><span> sklearn</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>metrics </span><span class="token" style="color:hsl(301, 63%, 40%)">import</span><span> accuracy_score</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>precision_score</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>recall_score
</span><span></span><span class="token" style="color:hsl(301, 63%, 40%)">from</span><span> sklearn</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>ensemble </span><span class="token" style="color:hsl(301, 63%, 40%)">import</span><span> GradientBoostingClassifier
</span><span></span><span class="token" style="color:hsl(301, 63%, 40%)">from</span><span> fairlearn</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>metrics </span><span class="token" style="color:hsl(301, 63%, 40%)">import</span><span> selection_rate
</span><span></span><span class="token" style="color:hsl(301, 63%, 40%)">from</span><span> fairlearn</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>reductions </span><span class="token" style="color:hsl(301, 63%, 40%)">import</span><span> ExponentiatedGradient</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> DemographicParity
</span><span>classifier </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> GradientBoostingClassifier</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>classifier</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>fit</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>X</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> y</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>y_pred </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> classifier</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>predict</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>X</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>metrics </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> </span><span class="token" style="color:hsl(230, 8%, 24%)">{</span><span>
</span><span>    </span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;accuracy&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">:</span><span> accuracy_score</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    </span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;precision&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">:</span><span> precision_score</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    </span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;recall&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">:</span><span> recall_score</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    </span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;selection_rate&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">:</span><span> selection_rate
</span><span></span><span class="token" style="color:hsl(230, 8%, 24%)">}</span><span>
</span><span>metric_frame </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> MetricFrame</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>metrics</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>metrics</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>                           y_true</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>y</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>                           y_pred</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>y_pred</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>                           sensitive_features</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>sex</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>metric_frame</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>by_group</span><span class="token" style="color:hsl(230, 8%, 24%)">[</span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;selection_rate&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">]</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>plot</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>bar</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>color</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> </span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;r&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> title</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span class="token" style="color:hsl(119, 34%, 47%)">&#x27;Selection Rate split by Sex&#x27;</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span></code></pre></div></pre>
<p><img src="https://miro.medium.com/max/1400/1*ILbeO5Vn4AsXPfgEZeLCiA.webp" alt="Selection Rate Split by Sex" node="[object Object]"/></p>
<p align="center"> Figure 4: Selection Rate Split by Sex </p>
<p>We see that the percentage of males having a salary above 50K is almost 3x the percentage of females. (!!)
Once the model is trained we see that this ratio is now 5x (!!). The model is exacerbating the bias we see in the data.</p>
<h3 id="32-correcting-bias">3.2. Correcting bias</h3>
<p>Let&#x27;s now correct the bias we observe by applying Demographic Parity on our classifier (we use ExponentiatedGradient from fairlearn for this). More context on how it works behind the scene can be found on the official fairlearn documentation here.</p>
<pre><div style="position:relative"><button aria-label="Copy code" style="position:absolute;top:8px;right:8px;z-index:10;background:none;border:none;padding:0;cursor:pointer;opacity:0.7;transition:opacity 0.2s"><svg width="20" height="20" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor"></rect><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path></svg></button><pre style="background:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:0.5em 0;overflow:auto;border-radius:0.3em"><code class="language-python" style="white-space:pre;background:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>np</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>random</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>seed</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span class="token" style="color:hsl(35, 99%, 36%)">0</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>  </span><span class="token" style="color:hsl(230, 4%, 64%);font-style:italic"># set seed for consistent results with ExponentiatedGradient</span><span>
</span>
<span>constraint </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> DemographicParity</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>classifier </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> GradientBoostingClassifier</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>mitigator </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> ExponentiatedGradient</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>classifier</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> constraint</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>mitigator</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>fit</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>X</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> y</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> sensitive_features</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>sex</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>y_pred_mitigated </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> mitigator</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>predict</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>X</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span>
<span>sr_mitigated </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> MetricFrame</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>metrics</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>selection_rate</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> y_true</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>y</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> y_pred</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>y_pred_mitigated</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> sensitive_features</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>sex</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(301, 63%, 40%)">print</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>sr_mitigated</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>overall</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(301, 63%, 40%)">print</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>sr_mitigated</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>by_group</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>metric_frame_mitigated </span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span> MetricFrame</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>metrics</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>metrics</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>                           y_true</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>y</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>                           y_pred</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>y_pred_mitigated</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>                           sensitive_features</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span>sex</span><span class="token" style="color:hsl(230, 8%, 24%)">)</span><span>
</span><span>metric_frame_mitigated</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>by_group</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>plot</span><span class="token" style="color:hsl(230, 8%, 24%)">.</span><span>bar</span><span class="token" style="color:hsl(230, 8%, 24%)">(</span><span>
</span><span>    subplots</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span class="token" style="color:hsl(35, 99%, 36%)">True</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    layout</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span class="token" style="color:hsl(230, 8%, 24%)">[</span><span class="token" style="color:hsl(35, 99%, 36%)">3</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> </span><span class="token" style="color:hsl(35, 99%, 36%)">3</span><span class="token" style="color:hsl(230, 8%, 24%)">]</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    legend</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span class="token" style="color:hsl(35, 99%, 36%)">False</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    figsize</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span class="token" style="color:hsl(230, 8%, 24%)">[</span><span class="token" style="color:hsl(35, 99%, 36%)">12</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span> </span><span class="token" style="color:hsl(35, 99%, 36%)">8</span><span class="token" style="color:hsl(230, 8%, 24%)">]</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span>    title</span><span class="token" style="color:hsl(221, 87%, 60%)">=</span><span class="token" style="color:hsl(119, 34%, 47%)">&quot;Show all metrics&quot;</span><span class="token" style="color:hsl(230, 8%, 24%)">,</span><span>
</span><span></span><span class="token" style="color:hsl(230, 8%, 24%)">)</span></code></pre></div></pre>
<p><img src="https://miro.medium.com/max/1400/1*9x24g1w-4HVrLr4nNQEPvw.webp" alt="Selection rate for original model vs mitigated one" node="[object Object]"/></p>
<p align="center"> Figure 5: Selection rate for original model vs mitigated one </p>
<p>By mitigating the model we introduced demographic parity (and thus equal selection rates) for our new model. Our model is now fair!!!</p>
<hr/>
<p>Woohoo! You now know the basics of fairness works and how you can start using it right away in your machine learning projects!</p>
<p>I hope you liked this article! Let me know if you have any questions or suggestions. Also feel free to contact me on LinkedIn, GitHub or Twitter, or checkout some other articles I wrote on DS/ML best practices. Happy learning!</p>
<p>Sources:</p>
<ul>
<li><a href="https://fairlearn.org/">https://fairlearn.org/</a></li>
</ul>
<h2 id="about-me">About me</h2>
<p>Hey! ðŸ‘‹ I&#x27;m Armand Sauzay (<a href="https://twitter.com/armandsauzay">armandsauzay</a>). You can find, follow or contact me on:</p>
<ul>
<li><a href="https://github.com/armand-sauzay">Github</a></li>
<li><a href="https://twitter.com/armandsauzay">Twitter</a></li>
<li><a href="https://www.linkedin.com/in/armand-sauzay-80a70b160/">LinkedIn</a></li>
<li><a href="https://medium.com/@armand-sauzay">Medium</a></li>
<li><a href="https://dev.to/armandsauzay">Dev.to</a></li>
</ul></article></main></div></main><footer class="w-full flex justify-center items-center gap-4 p-6 text-sm text-gray-500 mt-auto"><div class="flex gap-8 items-center "><a class="text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 " target="_blank" rel="noopener noreferrer" href="https://github.com/armand-sauzay"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-4 w-4" style="width:16px;height:16px"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg><span class="sr-only">GitHub</span></a><a class="text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 " target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/armandsauzay"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-4 w-4" style="width:16px;height:16px"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg><span class="sr-only">LinkedIn</span></a><a class="text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 " href="mailto:sauzayarmand@gmail.com"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail h-4 w-4" style="width:16px;height:16px"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg><span class="sr-only">Email</span></a></div><span class="text-gray-400">Â© armand sauzay</span></footer><script src="/_next/static/chunks/webpack-87b4446f5aacfc58.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48173,[\"173\",\"static/chunks/173-24113e259ce525da.js\",\"292\",\"static/chunks/app/articles/page-9a53c5301fbc680c.js\"],\"\"]\n3:I[15244,[],\"\"]\n4:I[43866,[],\"\"]\n6:I[86213,[],\"OutletBoundary\"]\n8:I[86213,[],\"MetadataBoundary\"]\na:I[86213,[],\"ViewportBoundary\"]\nc:I[34835,[],\"\"]\n:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/56593b524a1ee8d7.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Nup1GNXi89SCM3Cojs4nY\",\"p\":\"\",\"c\":[\"\",\"articles\",\"how-fair-are-your-machine-learning-models\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"articles\",{\"children\":[[\"slug\",\"how-fair-are-your-machine-learning-models\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/56593b524a1ee8d7.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_d65c78 min-h-screen flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"nav\",null,{\"className\":\"p-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto flex justify-between items-center\",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/\",\"className\":\"text-[#005530] font-semibold text-xl\",\"children\":\"armandsauzay\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-6\",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/articles\",\"className\":\"text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-book-open h-5 w-5\",\"children\":[[\"$\",\"path\",\"1akyts\",{\"d\":\"M12 7v14\"}],[\"$\",\"path\",\"ruj8y\",{\"d\":\"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"Articles\"}]]}],[\"$\",\"$L2\",null,{\"href\":\"/projects\",\"className\":\"text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-code-xml h-5 w-5\",\"children\":[[\"$\",\"path\",\"1inbqp\",{\"d\":\"m18 16 4-4-4-4\"}],[\"$\",\"path\",\"15zrgr\",{\"d\":\"m6 8-4 4 4 4\"}],[\"$\",\"path\",\"e7oirm\",{\"d\":\"m14.5 4-5 16\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"Projects\"}]]}],[\"$\",\"$L2\",null,{\"href\":\"/creative-lab\",\"className\":\"text-gray-600 hover:text-[#24512B] transition-colors flex items-center gap-2 cursor-pointer\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-brain h-5 w-5\",\"children\":[[\"$\",\"path\",\"l5xja\",{\"d\":\"M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z\"}],[\"$\",\"path\",\"ep3f8r\",{\"d\":\"M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z\"}],[\"$\",\"path\",\"1p4c4q\",{\"d\":\"M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4\"}],[\"$\",\"path\",\"tmeiqw\",{\"d\":\"M17.599 6.5a3 3 0 0 0 .399-1.375\"}],[\"$\",\"path\",\"105sqy\",{\"d\":\"M6.003 5.125A3 3 0 0 0 6.401 6.5\"}],[\"$\",\"path\",\"ql3yin\",{\"d\":\"M3.477 10.896a4 4 0 0 1 .585-.396\"}],[\"$\",\"path\",\"1qfode\",{\"d\":\"M19.938 10.5a4 4 0 0 1 .585.396\"}],[\"$\",\"path\",\"2e4loj\",{\"d\":\"M6 18a4 4 0 0 1-1.967-.516\"}],[\"$\",\"path\",\"159ez6\",{\"d\":\"M19.967 17.484A4 4 0 0 1 18 18\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"Creative Lab\"}]]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto border-b border-gray-200\"}]]}],[\"$\",\"main\",null,{\"className\":\"flex-1 flex flex-col items-center justify-center\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"w-full flex justify-center items-center gap-4 p-6 text-sm text-gray-500 mt-auto\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex gap-8 items-center \",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"https://github.com/armand-sauzay\",\"className\":\"text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 \",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-github h-4 w-4\",\"style\":{\"width\":16,\"height\":16},\"children\":[[\"$\",\"path\",\"tonef\",{\"d\":\"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4\"}],[\"$\",\"path\",\"9comsn\",{\"d\":\"M9 18c-4.51 2-5-2-7-2\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"GitHub\"}]]}],[\"$\",\"$L2\",null,{\"href\":\"https://linkedin.com/in/armandsauzay\",\"className\":\"text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 \",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-linkedin h-4 w-4\",\"style\":{\"width\":16,\"height\":16},\"children\":[[\"$\",\"path\",\"c2jq9f\",{\"d\":\"M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z\"}],[\"$\",\"rect\",\"mk3on5\",{\"width\":\"4\",\"height\":\"12\",\"x\":\"2\",\"y\":\"9\"}],[\"$\",\"circle\",\"bt5ra8\",{\"cx\":\"4\",\"cy\":\"4\",\"r\":\"2\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"LinkedIn\"}]]}],[\"$\",\"$L2\",null,{\"href\":\"mailto:sauzayarmand@gmail.com\",\"className\":\"text-gray-500 hover:text-[#24512B] transition-transform transform hover:scale-110 \",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-mail h-4 w-4\",\"style\":{\"width\":16,\"height\":16},\"children\":[[\"$\",\"rect\",\"18n3k1\",{\"width\":\"20\",\"height\":\"16\",\"x\":\"2\",\"y\":\"4\",\"rx\":\"2\"}],[\"$\",\"path\",\"1ocrg3\",{\"d\":\"m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Email\"}]]}]]}],[\"$\",\"span\",null,{\"className\":\"text-gray-400\",\"children\":\"Â© armand sauzay\"}]]}]]}]}]]}],{\"children\":[\"articles\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"articles\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"how-fair-are-your-machine-learning-models\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"articles\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":\"$L7\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"yGjTRJaq6c1S1TjckYvNT\",{\"children\":[[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Armand Sauzay\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Personal website of Armand Sauzay\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}]]\n"])</script><script>self.__next_f.push([1,"7:null\n"])</script><script>self.__next_f.push([1,"d:I[95958,[\"313\",\"static/chunks/313-6cc61c2fd8ca5084.js\",\"904\",\"static/chunks/app/articles/%5Bslug%5D/page-444914622aa9eeee.js\"],\"default\"]\ne:T2804,"])</script><script>self.__next_f.push([1,"# How Fair Are Your Machine LearningÂ Models?\n\nA quick introduction to the topic of fairness with hands on coding. Evaluate your machine learning model fairness in just a few lines ofÂ code.\n\nThis article lives on: \n- [Medium](https://armand-sauzay.medium.com/a-primer-on-machine-learning-fairness-using-fairlearn-8c00f92015dc)\n- [Dev.to](https://dev.to/armandsauzay/machine-learning-interpretability-and-feature-selection-made-easy-with-shap-450c)\n\n![Photo by Wesley Tingey onÂ Unsplash](https://miro.medium.com/max/1400/1*MZWIZHRWNqGkyoC_MKT-OA.webp)\n\u003cp align=\"center\"\u003e Photo by Wesley Tingey onÂ Unsplash \u003c/p\u003e\n\n\nAre Machine Learning models \"fair\"? When increasingly more decisions are backed by ML algorithms, it becomes important to understand the biases they can create.\n\nBut what does \"fairness\" mean? This is where it gets a little political (and mathematical)â€¦ To illustrate our thoughts, we'll take the example of a machine learning model which predicts whether a salary should be higher than 50K/year based on a number of features including __age__ and __gender__.\n\nAnd maybe you've already guessed, by looking at these two features, that fairness can have different definitions. Fair for __gender__ might mean that we want to have the a prediction which is independent of gender (i.e. paying the same people who only differ by their gender). Fair for __age__ might mean something else. We'd probably want to allow a certain correlation between the prediction and the age, as it seems fair to pay better older individuals (which usually are more experienced).\n\nOne key thing to understand is that what is judged \"fair\" is sometimes not even respected in the data itself.\n\n\u003e How would the model learn that men and women should be paid the same at equal levels it it does not observe this in the data itselfÂ ?\n\n![data biases vs modelÂ biases](https://miro.medium.com/max/1400/1*W_rBWQ-Z1Mw3sE43lDfNLw.webp)\n\u003cp align=\"center\"\u003e Figure1: data biases vs modelÂ biases \u003c/p\u003e\n\nNow that we have a bit of context on the problem, let's get into the math (Section 1) and the code (Sections 2 and 3)to be able to evaluate and address unfairness issues:\n\n1.  A few fairness concepts\n2. Evaluating Data Biases\n3. Evaluating and Correcting Model Biases with Fairlearn   \n    \n    a. Evaluating biasÂ \n\n    b. Correcting bias\n\n---\n\nAll the code for this tutorial can be found on Kaggle [here](https://www.kaggle.com/code/armandsauzay/a-primer-on-fairness-with-fairlearn). Feel free to run the notebook yourself or create aÂ copy!\n\n---\n\n## 1. A few fairness concepts\n### 1.1. Mathematical definition of fairness \nIn order to simplify things, we'll restrict the scope to binary classification (predict whether someone should be paid more than 50K/year).\nUsually, we'll call:\n- X: the feature matrix\n- Y: the target\n- A: Sensitive feature, usually one of the columns of X\n\nFor __binary classification__, two main definition of fairness exist:\n- __Demographic parity__ (also known as statistical parity): A classifier h satisfies demographic parity under a distribution over (X,A,Y) if its prediction h(X) is statistically independent of the sensitive feature A. This is equivalent to: `E[h(X)|A=a]=E[h(X)]`\n- __Equalized odds__: A classifier h satisfies equalized odds under a distribution over (X,A,Y) if its prediction h(X) is conditionally independent of the sensitive feature A given the label Y. This is equivalent to: `E[h(X)|A=a,Y=y]=E[h(X)|Y=y]`\n\n\u003e NOTE: a third one exists but is more rarely used: __equal opportunity__ is a relaxed version of equalized odds that only considers conditional expectations with respect to positive labels.\n\n### 1.2. Fairness in words\n\nIn \"simpler words\":\n- __Demographic parity__: the prediction should be independent from the sensitive features (for instance independent from gender). It states that all categories from the protected feature should receive the positive outcome at the same rate (it plays on selection rate)\n- __Equalized odds__: the prediction can be correlated to the sensitive feature, to the extent it is explained by the data we see\n\n### 1.3. Why does it matter?\n\n\u003e OK, that's interesting, but why does it matter? And how can I use those mathematical concepts?\n\nâ†’ Let's take two examples of features and then explain what type of fairness we want to have for this feature. Going back to the previous example of salary prediction, let's say you are the CEO of a very big company and want to build an algorithm which would give you the salary you should give to your employees based on performance indicators. Ideally you would look for something like:\n- Demographic Parity for gender: the salary prediction should be independent from the gender\n- Equalized Odds for Age: the salary prediction should not be independent from Age (you want to still pay more employees with more experience) but you still want to control that the salary so that you do not end up being too skewed â†’ you don't want to end up in the situation where the algorithm exacerbates even more the inequalities (pays the youth even less and the elders even more)\n\nWithout further due, let's get into the implementation details on how we can evaluate fairness and \"retrain\" our Machine Learning models against its biases. For this we're going to use the [UCI Adult Dataset](https://archive.ics.uci.edu/ml/datasets/adult).\n\n## 2. Evaluating DataÂ Biases\n_NOTE: once again, you can find all the associated code [here](https://www.kaggle.com/code/armandsauzay/a-primer-on-fairness-with-fairlearn)._ \n\nBiases can exist in the data itself. Let's just load the data and plot the percentage of Male/Female having a salary above 50K.\n\n\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1106/1*dMFb4gM4LjtQ5TC52fBTVA.webp\" width=\"400\" height=\"400\" /\u003e \u003cimg src=\"https://miro.medium.com/max/896/1*d5GJ2Z7Yz3uom5ANr96yGQ.webp\" width=\"400\" height=\"400\" /\u003e \u003c/p\u003e\n\u003cp align=\"center\"\u003e Figure 2: gender and age impact onÂ salary \u003c/p\u003e\n\nWe see that the percentage of males having a salary above 50K is almost 3x the percentage of females. (!!)\n\nIf the algorithm learns on this data it will definitely be biased. To counter this bias we can either:Â \n1. cherry pick data so that the percentage of maleÂ \n2. use fairlearn to correct the bias after the model is trained on this unfair data\n\nIn section 3, we'll focus on the second approach.\n\n\n## 3. Evaluating and Correcting Model Biases with Fairlearn\n### 3.1. Evaluating bias\nOne of the most interesting features here is probably __selection rate__. It is the rate of predicting positive outcomes (in this case, whether salary is above 50K)\n\n\u003cp align=\"center\"\u003e \u003cimg src=\"https://miro.medium.com/max/708/1*oflHCW6PPNRa2Eb0yAIjvA.webp\" /\u003e \u003c/p\u003e\n\u003cp align=\"center\"\u003e Figure 3: Selection Rate Definition \u003c/p\u003e\n\nLet's use MetricFrame from fairlearn to calculate the selection rates split by Sex.\n\n```python\nfrom fairlearn.metrics import MetricFrame\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom fairlearn.metrics import selection_rate\nfrom fairlearn.reductions import ExponentiatedGradient, DemographicParity\nclassifier = GradientBoostingClassifier()\nclassifier.fit(X, y)\ny_pred = classifier.predict(X)\nmetrics = {\n    'accuracy': accuracy_score,\n    'precision': precision_score,\n    'recall': recall_score,\n    'selection_rate': selection_rate\n}\nmetric_frame = MetricFrame(metrics=metrics,\n                           y_true=y,\n                           y_pred=y_pred,\n                           sensitive_features=sex)\nmetric_frame.by_group['selection_rate'].plot.bar(color= 'r', title='Selection Rate split by Sex')\n```\n\n![Selection Rate Split by Sex](https://miro.medium.com/max/1400/1*ILbeO5Vn4AsXPfgEZeLCiA.webp)\n\u003cp align='center'\u003e Figure 4: Selection Rate Split by Sex \u003c/p\u003e\n\nWe see that the percentage of males having a salary above 50K is almost 3x the percentage of females. (!!)\nOnce the model is trained we see that this ratio is now 5x (!!). The model is exacerbating the bias we see in the data.\n\n### 3.2. Correcting bias\nLet's now correct the bias we observe by applying Demographic Parity on our classifier (we use ExponentiatedGradient from fairlearn for this). More context on how it works behind the scene can be found on the official fairlearn documentation here.\n\n```python\nnp.random.seed(0)  # set seed for consistent results with ExponentiatedGradient\n\nconstraint = DemographicParity()\nclassifier = GradientBoostingClassifier()\nmitigator = ExponentiatedGradient(classifier, constraint)\nmitigator.fit(X, y, sensitive_features=sex)\ny_pred_mitigated = mitigator.predict(X)\n\nsr_mitigated = MetricFrame(metrics=selection_rate, y_true=y, y_pred=y_pred_mitigated, sensitive_features=sex)\nprint(sr_mitigated.overall)\nprint(sr_mitigated.by_group)\nmetric_frame_mitigated = MetricFrame(metrics=metrics,\n                           y_true=y,\n                           y_pred=y_pred_mitigated,\n                           sensitive_features=sex)\nmetric_frame_mitigated.by_group.plot.bar(\n    subplots=True,\n    layout=[3, 3],\n    legend=False,\n    figsize=[12, 8],\n    title=\"Show all metrics\",\n)\n```\n![Selection rate for original model vs mitigated one](https://miro.medium.com/max/1400/1*9x24g1w-4HVrLr4nNQEPvw.webp)\n\u003cp align='center'\u003e Figure 5: Selection rate for original model vs mitigated one \u003c/p\u003e\n\nBy mitigating the model we introduced demographic parity (and thus equal selection rates) for our new model. Our model is now fair!!!\n\n---\n\nWoohoo! You now know the basics of fairness works and how you can start using it right away in your machine learning projects!\n\nI hope you liked this article! Let me know if you have any questions or suggestions. Also feel free to contact me on LinkedIn, GitHub or Twitter, or checkout some other articles I wrote on DS/ML best practices. Happy learning!\n\nSources:\n- https://fairlearn.org/\n\n## About me\nHey! ðŸ‘‹ I'm Armand Sauzay ([armandsauzay](https://twitter.com/armandsauzay)). You can find, follow or contact me on: \n\n- [Github](https://github.com/armand-sauzay) \n- [Twitter](https://twitter.com/armandsauzay)\n- [LinkedIn](https://www.linkedin.com/in/armand-sauzay-80a70b160/)\n- [Medium](https://medium.com/@armand-sauzay)\n- [Dev.to](https://dev.to/armandsauzay)"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-1 max-w-4xl mx-auto px-4 py-16\",\"children\":[\"$\",\"$Ld\",null,{\"markdown\":\"$e\",\"markdownUrl\":\"https://raw.githubusercontent.com/armand-sauzay/blog-posts/main/how-fair-are-your-machine-learning-models/README.md\"}]}]}]\n"])</script></body></html>