<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8" />
  <title>Armand Sauzay</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  
  <link rel="stylesheet" href="/css/output.css">
  
  
  <link rel="stylesheet" href="/css/syntax.css">
  
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body class="min-h-screen flex flex-col font-sans">
  <header>
  <div class="max-w-5xl mx-auto flex items-center justify-between px-6 py-5 border-b border-gray-200">
    <a href="/" class="text-green-800 font-bold text-xl hover:text-green-900">armandsauzay</a>
    <nav class="flex items-center space-x-8">
      <a href="/articles" class="flex items-center gap-1 text-gray-700 hover:text-green-800">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-book-open-icon lucide-book-open"><path d="M12 7v14"/><path d="M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z"/></svg>
        Articles
      </a>
      <a href="/projects" class="flex items-center gap-1 text-gray-700 hover:text-green-800">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-code-xml-icon lucide-code-xml"><path d="m18 16 4-4-4-4"/><path d="m6 8-4 4 4 4"/><path d="m14.5 4-5 16"/></svg>
        Projects
      </a>
      <a href="/creative-lab" class="flex items-center gap-1 text-gray-700 hover:text-green-800">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-brain-icon lucide-brain"><path d="M12 5a3 3 0 1 0-5.997.125 4 4 0 0 0-2.526 5.77 4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z"/><path d="M12 5a3 3 0 1 1 5.997.125 4 4 0 0 1 2.526 5.77 4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z"/><path d="M15 13a4.5 4.5 0 0 1-3-4 4.5 4.5 0 0 1-3 4"/><path d="M17.599 6.5a3 3 0 0 0 .399-1.375"/><path d="M6.003 5.125A3 3 0 0 0 6.401 6.5"/><path d="M3.477 10.896a4 4 0 0 1 .585-.396"/><path d="M19.938 10.5a4 4 0 0 1 .585.396"/><path d="M6 18a4 4 0 0 1-1.967-.516"/><path d="M19.967 17.484A4 4 0 0 1 18 18"/></svg>
        Creative Lab
      </a>
    </nav>
  </div>
</header>
  <main class="flex flex-1 flex-col">
    
<article class="mx-auto px-6 py-8" style="max-width: 44rem;">
  <header class="mb-12">
    <h1 class="text-4xl font-bold text-gray-900 mb-6">SHAP values: Machine Learning Interpretability Made Easy</h1>
    
    
    
    
    
    <div class="mb-6">
      
<div class="flex items-center text-sm text-gray-500">
  <span>25 Jun 2022</span>
  <span class="px-2">•</span>
  <span>5 min read</span>
  
  <span class="px-2">•</span>
  <div class="flex flex-wrap gap-2">
    
    <span class="bg-gray-100 px-2 py-1 rounded text-xs">machinelearning</span>
    
    <span class="bg-gray-100 px-2 py-1 rounded text-xs">interpretability</span>
    
  </div>
  
</div>
    </div>
  </header>
  
  <div class="article-content">
    <p>Machine Learning interpretability is becoming increasingly important, especially as ML algorithms are getting more complex.</p>
<p>How good is your Machine Learning algorithm if it cant be explained? Less performant but explainable models (like linear regression) are sometimes preferred over more performant but black box models (like XGBoost or Neural Networks). This is why research around machine learning explainability (aka eXplainable AI or XAI) has recently been a growing field with amazing projects like SHAP emerging.</p>
<blockquote>
<p>Would you feel confident using a machine learning model if you can&rsquo;t explain what it does?</p></blockquote>
<p>This is where SHAP can be of great help: it can explain <strong>any ML model</strong> by giving the influence of each of the features on the target. But this is not all that SHAP can do.</p>
<p>Build a simple model (sklearn/xgboost/keras) and use SHAP: you now have a feature selection process by looking at features which have the biggest impact on the prediction.
But how does SHAP work under the hood? And how can you start using it?</p>
<p>In this article we&rsquo;ll first get our hands on some python code to see how you can start using SHAP and how it can help you both for explainability and feature selection.</p>
<p>Then, for those of you who want to get into the details of SHAP, we&rsquo;ll go through the theory behind popular XAI tools like SHAP and LIME.</p>
<hr>
<p>All the code for this tutorial can be found on Kaggle <a href="https://www.kaggle.com/code/armandsauzay/shap-interpret-any-ml-model-feature-selection?scriptVersionId=99746743">here</a>. Feel free to run the notebook yourself or create a copy!</p>
<hr>
<h2 id="1-how-can-you-start-usingshap">1. How can you start using SHAP?</h2>
<p>Here, we&rsquo;ll go through a simple example with Shap values using the competition Kaggle competition &ldquo;House Prices - Advanced Regression Techniques&rdquo; to illustrate SHAP. If you are interested or you have never been on Kaggle before, feel free to read more about the data and the competition itself here.</p>
<p>The process to use shap is quite straightforward: we need to build a model and then use the shap library to explain it.
<img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BG6W1B8OW6gUVHPbBXIvIA.png" alt="Explanation of how to use SHAP in your Machine Learning Project"></p>
<p>understand the output of your modelHere, our machine learning model tries to predict the house prices from the data that is given (number of square feet, quality, number of floors etc).</p>
<p>The usual workflow in terms of code looks like this:</p>
<ol>
<li>Create an estimator. For instance GradientBoostingRegressor from sklearn.ensemble:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>Train your estimator:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>use shap library to calculate the SHAP values. For instance, using the following code:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">Explainer</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="n">X100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X100</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>See what is the impact of each feature using shap.summary_plot:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div></li>
</ol>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9M6mMjsGH_nj0BG_7uJpCg.png" alt="Summary plot of feature importance"></p>
<p>For instance, you can see here that OverallQual is the feature that has the most impact on the model output. High values (colored in red on the graph above) of OverallQual can increase a property&rsquo;s price by ~60,000 and low values can decrease a price by ~20,000. Interesting to know if you&rsquo;re in real estate, isn&rsquo;t it?</p>
<p>But this is not all of what SHAP can do! SHAP can also explain a single prediction.</p>
<p>For example, using shap.plots.waterfall for a single element in the dataset, you can have the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">waterfall</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="n">sample_index</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGEnQK781FHnjEwexOklbg.png" alt="Waterfall plot of feature importance for a single example"></p>
<p>For this specific example, the predicted price was 166k (vs 174k on average). And we can understand why the algorithm predicted such: for instance OverallQual which is high (7) drives the value up but YearBuilt (1925) drives the value down.</p>
<p>You can now understand the dynamics behind your model, both overall and on specific datapoints. With SHAP, you can more easily see if something is wrong (or does not make sense for your sharpened data science mind) so you can correct it! This is what observability is about.</p>
<p>And since SHAP allows you to understand the feature importance of your model, you can also use this for feature selection. For instance</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPp8lnK6n2peGk1wbBb8Og.png" alt="Barplot of feature importance"></p>
<p>Then you can see which features do not have a lot of impact on the output of the model. The features usually are noise for your machine learning model and do not bring a lot of predictive value. So removing them from your training set will generally improve the performance, and allow you to tune correctly the hyper parameters without overfitting on noisy data.
<img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zI3DWLj7E71y4jyVHHJx9g.png" alt="Worfklow for feature selection in your machine learning project"></p>
<p align="center"> How SHAP can be used for feature selection </p>
<h2 id="2-quick-overview-how-does-shap-work-under-thehood">2. Quick overview: How does SHAP work under the hood?</h2>
<p>If you have a bit of time, feel free to read the original paper that describes the different approaches for model explainability and goes through the advantages of SHAP.</p>
<p>But let&rsquo;s try to explain in short what SHAP is doing and the concepts behind without getting too deep into the mathematical equations.</p>
<p>Explainable Machine Learning (aka eXplainable AI or XAI) aims at understanding why the output of a machine learning model is such. To do so, you could theoretically take the definition of your model, for example a tree based model like Random Forest, and then see why your output is such. But this is not so straightforward, and of course, it gets even more complex for Deep Learning models…</p>
<p>Instead of going through the winding path of understanding what happens inside your model (forward and backward propagation for deep learning models, which splits are the most used in your RF algo etc). But once you have your trained model, could you not instead use it to see how it reacts when you change a feature?</p>
<p>This is the core concept behind popular XAI algorithm (SHAP, LIME etc): use your existing model, approximate it using an explainable model and you now have an explainable model. The complexity is now on how to approximate a ML model around a given prediction, and then around most predictions.</p>
<p>Sources:</p>
<ul>
<li><a href="https://shap.readthedocs.io/en/latest/index.html">official documentation</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf">original paper</a></li>
</ul>

  </div>
</article>

  </main>
  <footer class="py-5 text-center text-gray-500 text-sm mt-auto">
  <div class="mx-auto flex justify-center items-center gap-8 px-4">
    <div class="flex gap-8 items-center">
      <a href="https://github.com/armandsauzay" target="_blank" class="text-gray-500 hover:text-green-800" aria-label="GitHub">
        
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github-icon lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></svg>
      </a>
      <a href="https://linkedin.com/in/armandsauzay" target="_blank" class="text-gray-500 hover:text-green-800" aria-label="LinkedIn">
        
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin-icon lucide-linkedin"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"/><rect width="4" height="12" x="2" y="9"/><circle cx="4" cy="4" r="2"/></svg>
      </a>
      <a href="mailto:sauzayarmand@gmail.com" class="text-gray-500 hover:text-green-800" aria-label="Email">
        
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail-icon lucide-mail"><path d="m22 7-8.991 5.727a2 2 0 0 1-2.009 0L2 7"/><rect x="2" y="4" width="20" height="16" rx="2"/></svg>
      </a>
    </div>
    <div class="font-medium text-sm">© armandsauzay</div>
  </div>
</footer>
</body>
</html>
