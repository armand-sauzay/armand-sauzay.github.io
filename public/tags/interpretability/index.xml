<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Interpretability on Armand Sauzay</title>
    <link>//localhost:1313/tags/interpretability/</link>
    <description>Recent content in Interpretability on Armand Sauzay</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 Jun 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/tags/interpretability/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SHAP values: Machine Learning Interpretability Made Easy</title>
      <link>//localhost:1313/articles/shap-values/</link>
      <pubDate>Sat, 25 Jun 2022 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/articles/shap-values/</guid>
      <description>&lt;p&gt;Machine Learning interpretability is becoming increasingly important, especially as ML algorithms are getting more complex.&lt;/p&gt;&#xA;&lt;p&gt;How good is your Machine Learning algorithm if it cant be explained? Less performant but explainable models (like linear regression) are sometimes preferred over more performant but black box models (like XGBoost or Neural Networks). This is why research around machine learning explainability (aka eXplainable AI or XAI) has recently been a growing field with amazing projects like SHAP emerging.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
